general:
  name: NeRF-${dataset.case_name}
  result_dir: ./exp/${general.name}
  save_list: [./configs, ./datasets, ./models, ./utils, ./systems, ./launch.py]
  system: nerf

dataset:
  name: dtu
  root_dir: data/neus/
  case_name: dtu_scan24
  img_downscale: 1.0
  batch_over_images: true
  background_color: random  # support [white, black, random]
  apply_mask: true
  num_workers: 16  # the number of threads for preload
  val_images: [0, 1]

# todo: 改成类tcnn的配置
model:
  name: NeRF
  geometry:
    radius: 1.0
    feature_dim: 16
    density_activation: trunc_exp
    xyz_encoding_config:
      otype: HashGrid
      n_levels: 16
      n_features_per_level: 2
      log2_hashmap_size: 19
      base_resolution: 16
      per_level_scale: 1.447269237440378
    mlp_network_config:
      otype: FullyFusedMLP
      activation: ReLU
      output_activation: none
      n_neurons: 64
      n_hidden_layers: 1
  color:
    dir_encoding_config:
      otype: SphericalHarmonics
      degree: 4
    mlp_network_config:
      otype: FullyFusedMLP
      activation: ReLU
      output_activation: Sigmoid
      n_neurons: 64
      n_hidden_layers: 2
      include_xyz: false
      include_normal: false
  background: # todo: background model
    enabled: False
  render:
    train_num_rays: 256
    num_samples_per_ray: 1024
    max_train_num_rays: 8192
    dynamic_ray_sampling: true
    cos_anneal_end: ${trainer.max_steps}
    ray_chunk: 32768 # batch size for rendering the whole image
    sampler:
      type: OccGrid
      scene_aabb: ${model.geometry.radius}
      resolution: 128
      levels: 1  # 多网格, 从基础aabb开始的2的指数倍增加大小(1, 2, 4, 8)
      render_step_size: 5e-3
      grid_prune_occ_thre: 1e-2
    isosurface:
      resolution: 256
      threshold: 5.
      block_size: 64
      export_color: false
      cuda: true  
optimize:
  loss:
    rgb_weight: 1.0
    # distortion_weight: 0.01
  optimizer:
    name: FusedAdam # AdamW  # FusedAdam
    args:
      lr: 0.01  # 公用优化器时，参数未指定学习率使用公用的学习率
      betas: [0.9, 0.99]
      eps: 1.e-15
      weight_decay: 0.01
      amsgrad: false
    params:
      geometry:
        lr: 0.01
      color:
        lr: 0.01
  warmup_steps: 200
  scheduler:
    name: SequentialLR
    interval: step
    milestones:
      - ${optimize.warmup_steps}
      - 4000 
      - 10000
    schedulers:
      - name: LinearLR
        args:
          start_factor: 0.002 
          end_factor: 1.0
          total_iters: ${optimize.warmup_steps}
      - name: ConstantLR
        args:
          factor: 1.0
          total_iters: ${get:${optimize.scheduler.milestones}, 1} # 4000 
      - name: CosineAnnealingLR
        args: 
          T_max: ${sub:${get:${optimize.scheduler.milestones}, 2}, ${get:${optimize.scheduler.milestones}, 1}} # ${sub:${trainer.max_steps}, ${get:${optimize.scheduler.milestones}, 1}}
          eta_min: 0.0005
      - name: ConstantLR
        args:
          factor: 0.05
          total_iters: ${sub:${trainer.max_steps}, ${get:${optimize.scheduler.milestones}, 2}}     

checkpoint:
  save_top_k: -1
  every_n_train_steps: ${trainer.max_steps}
  
trainer:
  max_steps: 15000
  log_every_n_steps: 100
  num_sanity_val_steps: 0
  val_check_interval: 100000 # ${trainer.max_steps}
  limit_train_batches: 1.0
  limit_val_batches: ${len:${dataset.val_images}} # 2
  enable_progress_bar: true
  precision: 16